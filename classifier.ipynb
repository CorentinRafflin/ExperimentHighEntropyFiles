{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Forensics, project F3 </h3>\n",
    "<h1>Experiments on high entropy files</h1>\n",
    "<hr style=\"height:2px;border:none;color:#333;background-color:#333;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Author_\n",
    "<div class=\"alert alert-warning\">RAFFLIN Corentin </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5) Classifier construction </h2>\n",
    "\n",
    "The results of the experiments are saved in a file named `results.csv`, this notebook focuses on the processing of the data and the building of a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>1. Loading and treating the data</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diverses libraries\n",
    "%matplotlib inline\n",
    "import random\n",
    "from time import time\n",
    "import pickle\n",
    "# Data and plotting imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Neural network libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "#statistical libraries\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler \n",
    "from sklearn.model_selection import train_test_split# GridSearchCV,  KFold\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Loading the data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the CSV file\n",
    "resultsPath = 'results.csv'\n",
    "\n",
    "#Header to associate to the CSV file\n",
    "tests = ['File_type','File_bytes','Entropy','Chi_square','Mean','Monte_Carlo_Pi','Serial_Correlation'] \n",
    "cols = tests + [str(i) for i in range(0,256)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6220 files analyzed\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "data = pd.read_csv(resultsPath, sep=',', header=None, names=cols)\n",
    "print('There are {} files analyzed'.format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Removing outliers and balancing the data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count_before</th>\n",
       "      <th>Count_After</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pdf</th>\n",
       "      <td>1613</td>\n",
       "      <td>1140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jpg</th>\n",
       "      <td>1401</td>\n",
       "      <td>1137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>png</th>\n",
       "      <td>1136</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp3</th>\n",
       "      <td>1035</td>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <td>1035</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Count_before  Count_After\n",
       "pdf          1613         1140\n",
       "jpg          1401         1137\n",
       "png          1136         1103\n",
       "mp3          1035         1029\n",
       "zip          1035         1033"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "countBefore = data['File_type'].value_counts().to_frame().rename(index=str, columns={'File_type':'Count_before'})\n",
    "\n",
    "#Removing outliers by keeping only files with high entropy\n",
    "data = data[data.Entropy>7.6]\n",
    "\n",
    "countAfter = data['File_type'].value_counts().to_frame().rename(index=str, columns={'File_type':'Count_After'})\n",
    "\n",
    "count = pd.concat([countBefore, countAfter], axis=1, sort=False)\n",
    "display(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File types : ['jpg' 'mp3' 'pdf' 'png' 'zip']\n"
     ]
    }
   ],
   "source": [
    "#List of each file type\n",
    "file_types = data['File_type'].sort_values().unique()\n",
    "\n",
    "#List of dataframe for each file type \n",
    "files = [ data[data.File_type==file_type]  for file_type in file_types]\n",
    "\n",
    "#Colors to associate to the file types\n",
    "colors = ['r', 'b', 'g', 'y', 'm']\n",
    "\n",
    "# In case more colors are needed for addition of other file type\n",
    "'''\n",
    "colors = list(pltcolors._colors_full_map.values())\n",
    "random.seed(2)\n",
    "random.shuffle(colors)\n",
    "'''\n",
    "\n",
    "print(\"File types :\", file_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5145 files analyzed\n"
     ]
    }
   ],
   "source": [
    "#Removing some data (lower entropy) to have the same count for each file type\n",
    "minCount = data['File_type'].value_counts().iloc[-1]\n",
    "for i in range(len(files)):\n",
    "    f = files[i]\n",
    "    f = f.sort_values(by=\"Entropy\")\n",
    "    files[i] = f[len(f)-minCount:]\n",
    "\n",
    "#Updating the full dataframe\n",
    "data = pd.concat(files)\n",
    "print('There are {} files analyzed'.format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Checking for missing (possible errors) </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMissing(dataframe):\n",
    "    ''' Printing the missing data in the dataframe with the total of missing and the corresponding percentage '''\n",
    "    total = dataframe.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (dataframe.isnull().sum()/dataframe.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing_data[missing_data['Total']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Total, Percent]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Checking for missing in the tests or bytes distribution\n",
    "display(getMissing(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data in the tests which is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>2. Data Pre-processing</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will prepare the data for the clasifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Dropping and separating input-ouput </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping not useful information \n",
    "data = data.drop('File_bytes', axis=1)\n",
    "\n",
    "#Separating the output\n",
    "y = data['File_type']\n",
    "data = data.drop('File_type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Splitting into training and testing sets </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, y, test_size = 0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standardization </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b> standardization </b> of a dataset is a common requirement for many machine learning estimators. We use the RobustScaler more robust to outliers as it is possible that we have many outliers in this data set. The centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers. The outliers themselves are still present in the transformed data. \n",
    "\n",
    "> Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling features using statistics that are robust to outliers.\n",
    "scaler = RobustScaler() \n",
    "#Fitting on the training set, then transforming both training and testing sets\n",
    "X_train = scaler.fit(X_train).transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need for dimensionality reduction nor for decorrelating the data using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Encoding the output </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbencoder = LabelEncoder()\n",
    "lbencoder.fit(Y_train)\n",
    "Y_train = lbencoder.transform(Y_train)\n",
    "Y_test = lbencoder.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape x_train (4630, 261)\n",
      "Shape y_train (4630,)\n",
      "Shape x_test (515, 261)\n",
      "Shape y_test (515,)\n"
     ]
    }
   ],
   "source": [
    "#Printing the shapes\n",
    "print(\"Shape x_train\", X_train.shape)\n",
    "print(\"Shape y_train\", Y_train.shape)\n",
    "print(\"Shape x_test\", X_test.shape)\n",
    "print(\"Shape y_test\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>3. Model Selection</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several classifiers could be used for this problem. I decided to focus on SVM which is good for limited data and a Neural Network (Multi Layer Perceptron Classifier) which is fast for prediction and therefore could be better to implement in the `ent` program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In short:\n",
    "* Boosting - often effective when a large amount of training data is available.\n",
    "* Random trees - often very effective and can also perform regression.\n",
    "* K-nearest neighbors - simplest thing you can do, often effective but slow and requires lots of memory.\n",
    "* Neural networks - Slow to train but very fast to run, still optimal performer for letter recognition.\n",
    "* SVM - Among the best with limited data, but losing against boosting or random trees only when large data sets are available.\n",
    "https://stackoverflow.com/questions/2595176/which-machine-learning-classifier-to-choose-in-general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I will not focus on the optimization of the parameters.\n",
    "\n",
    "I used the f1_score as a metric to give weights to all classes and see the accuracy for each class.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> a. MultiLayer Perceptron (MLP) </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MLPs are suitable for classification prediction problems where inputs are assigned a class or label. They are very flexible and can be used generally to learn a mapping from inputs to outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='adam',hidden_layer_sizes=(50), random_state=1, max_iter=50000, activation='relu', \n",
    "                    learning_rate_init=0.00001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=50, learning_rate='constant',\n",
       "       learning_rate_init=1e-05, max_iter=50000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.982721\n",
      "Testing set score: 0.941748\n",
      "F1 test set score: [0.95789474 1.         0.98039216 0.87700535 0.87628866]\n",
      "Corresponding classes: ['jpg' 'mp3' 'pdf' 'png' 'zip']\n",
      "F1 mean test set score: 0.9383161802184494\n"
     ]
    }
   ],
   "source": [
    "#Results \n",
    "print(\"Training set score: %f\" % mlp.score(X_train, Y_train))\n",
    "print(\"Testing set score: %f\" % mlp.score(X_test, Y_test))\n",
    "Y_pred = mlp.predict(X_test)\n",
    "print(\"F1 test set score:\", metrics.f1_score(Y_test, Y_pred , average=None))\n",
    "print(\"Corresponding classes:\", lbencoder.classes_)\n",
    "print(\"F1 mean test set score:\", metrics.f1_score(Y_test, Y_pred , average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> b. SVC </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SVC and NuSVC implement the “one-against-one” approach (Knerr et al., 1990) for multi- class classification. If n_class is the number of classes, then n_class * (n_class - 1) / 2 classifiers are constructed and each one trains data from two classes. \n",
    "https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "A one against one approach may be better to differentiate two close classes like zip and png.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma='scale', decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.964579\n",
      "Testing set score: 0.953398\n",
      "F1 test set score: [0.97916667 1.         0.98007968 0.89617486 0.9       ]\n",
      "Corresponding classes: ['jpg' 'mp3' 'pdf' 'png' 'zip']\n",
      "F1 mean test set score: 0.951084242265909\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "print(\"Training set score: %f\" % svc.score(X_train, Y_train))\n",
    "print(\"Testing set score: %f\" % svc.score(X_test, Y_test))\n",
    "Y_pred = svc.predict(X_test)\n",
    "print(\"F1 test set score:\", metrics.f1_score(Y_test, Y_pred , average=None))\n",
    "print(\"Corresponding classes:\", lbencoder.classes_)\n",
    "print(\"F1 mean test set score:\", metrics.f1_score(Y_test, Y_pred , average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>4. Parameter Optimisation</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> a. MultiLayer Perceptron (MLP) </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kept the activation function `relu` which from experience and theory gives good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solver</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>lr</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "      <th>f1_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.957282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>50</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.957282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.954074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.953398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.953398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.939806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   solver hidden_layer_sizes       lr  test_score  train_score   f1_mean\n",
       "5   lbfgs                 50  0.00005    0.957282          1.0  0.954074\n",
       "7   lbfgs                 50  0.00010    0.957282          1.0  0.954074\n",
       "9   lbfgs                100  0.00005    0.953398          1.0  0.950701\n",
       "11  lbfgs                100  0.00010    0.953398          1.0  0.950701\n",
       "1   lbfgs                 20  0.00005    0.939806          1.0  0.935572"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the hyperparameters\n",
    "hyperparameters = {\n",
    "    'solver': ['adam', 'lbfgs'], \n",
    "    'hidden_layer_sizes' : [(20), (50), (100), (10,10)], \n",
    "    'lr' : [0.00005, 0.0001]\n",
    "}\n",
    "\n",
    "# Compute all combinations\n",
    "parameter_grid = list(ParameterGrid(hyperparameters))\n",
    "\n",
    "# Just a table to save the results\n",
    "resultsDF = pd.DataFrame(columns=['solver', 'hidden_layer_sizes', 'lr', 'test_score', 'train_score', 'f1_mean'])\n",
    "\n",
    "for p in parameter_grid:   \n",
    "    mlp = MLPClassifier(solver=p['solver'],hidden_layer_sizes=p['hidden_layer_sizes'], random_state=1, max_iter=50000,\n",
    "                        activation='relu', learning_rate_init=p['lr'], early_stopping=True, tol=1e-7 )\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    \n",
    "    test_score = mlp.score(X_test, Y_test)    \n",
    "    p['test_score'] = test_score\n",
    "    \n",
    "    train_score = mlp.score(X_train, Y_train)    \n",
    "    p['train_score'] = train_score\n",
    "    \n",
    "    Y_pred = mlp.predict(X_test)\n",
    "    f1_mean = metrics.f1_score(Y_test, Y_pred , average='macro')\n",
    "    p['f1_mean']=f1_mean\n",
    "    \n",
    "    resultsDF = resultsDF.append(p, ignore_index=True)\n",
    "    \n",
    "display(resultsDF.sort_values('test_score', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> b. SVC </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>kernel</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "      <th>f1_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.955340</td>\n",
       "      <td>0.989201</td>\n",
       "      <td>0.952933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.953398</td>\n",
       "      <td>0.964579</td>\n",
       "      <td>0.951084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.951456</td>\n",
       "      <td>0.948596</td>\n",
       "      <td>0.949330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.939806</td>\n",
       "      <td>0.988769</td>\n",
       "      <td>0.935264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.926214</td>\n",
       "      <td>0.992441</td>\n",
       "      <td>0.920596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C  kernel  test_score  train_score   f1_mean\n",
       "6  5.0     rbf    0.955340     0.989201  0.952933\n",
       "3  1.0     rbf    0.953398     0.964579  0.951084\n",
       "0  0.5     rbf    0.951456     0.948596  0.949330\n",
       "1  0.5  linear    0.939806     0.988769  0.935264\n",
       "4  1.0  linear    0.926214     0.992441  0.920596"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the hyperparameters\n",
    "hyperparameters = {\n",
    "    'C':[0.5, 1, 5],\n",
    "    'kernel':['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "# Compute all combinations\n",
    "parameter_grid = list(ParameterGrid(hyperparameters))\n",
    "\n",
    "# Just a table to save the results\n",
    "resultsDF = pd.DataFrame(columns=['C', 'kernel', 'test_score', 'train_score', 'f1_mean'])\n",
    "\n",
    "for p in parameter_grid:   \n",
    "    svc = SVC(gamma='scale', C=p['C'], kernel=p['kernel'], decision_function_shape='ovo')\n",
    "    svc.fit(X_train, Y_train)\n",
    "    \n",
    "    test_score = svc.score(X_test, Y_test)    \n",
    "    p['test_score'] = test_score\n",
    "    \n",
    "    train_score = svc.score(X_train, Y_train)    \n",
    "    p['train_score'] = train_score\n",
    "    \n",
    "    Y_pred = svc.predict(X_test)\n",
    "    f1_mean = metrics.f1_score(Y_test, Y_pred , average='macro')\n",
    "    p['f1_mean']=f1_mean\n",
    "    \n",
    "    resultsDF = resultsDF.append(p, ignore_index=True)\n",
    "    \n",
    "display(resultsDF.sort_values('test_score', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>5. Final Model </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though there are not big differences of accuracy for these two models, MLP classifier has slighlty better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=50, learning_rate='constant',\n",
       "       learning_rate_init=5e-06, max_iter=60000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=1e-07,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(50), random_state=1, max_iter=60000, activation='relu', \n",
    "                learning_rate_init=0.000005, early_stopping=True, tol=1e-7)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000000\n",
      "Testing set score: 0.957282\n",
      "F1 test set score: [0.95789474 1.         0.99224806 0.91397849 0.90625   ]\n",
      "Corresponding classes: ['jpg' 'mp3' 'pdf' 'png' 'zip']\n",
      "F1 mean test set score: 0.954074258696253\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "print(\"Training set score: %f\" % clf.score(X_train, Y_train))\n",
    "print(\"Testing set score: %f\" % clf.score(X_test, Y_test))\n",
    "Y_pred = clf.predict(X_test)\n",
    "f1_score = metrics.f1_score(Y_test, Y_pred, average=None)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"F1 test set score:\", metrics.f1_score(Y_test, Y_pred , average=None))\n",
    "print(\"Corresponding classes:\", lbencoder.classes_)\n",
    "print(\"F1 mean test set score:\", metrics.f1_score(Y_test, Y_pred , average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the mp3 class is always correctly predicted, and almost always for pdf. It was to be expected with the distributions insofar as most of their distributions were distinct from the others.  \n",
    "Similarly, we notice that png and zip have the lowest accuracy which is certainly due to the fact that they are difficult to distinguish and therefore the classifier may make mistakes between these two classes.\n",
    "\n",
    "There is a bit of overfitting as the testing set accuracy does not reach a perfect accuracy such as the training set. We would need to increase the number of samples for each class to improve the accuracy on the testing set.\n",
    "\n",
    "If we added more classes, it is likely that the global accuracy would lower due to similarity between some file types like we have for png and zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Saving </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"scaler_lb_clf.sav\"\n",
    "modlist = [scaler, lbencoder, clf]\n",
    "s = pickle.dump(modlist, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
